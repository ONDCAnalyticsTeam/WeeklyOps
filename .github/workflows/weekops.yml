name: Run Athena Query

on:
  workflow_dispatch:

jobs:
  run-query:
    runs-on: ubuntu-latest  # or windows-latest, if you prefer
    steps:
      # 1) Check out your repo to access the script and queries/
      - name: Check out repository
        uses: actions/checkout@v3

      # 2) Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      # 3) Install dependencies
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pandas pyathena

      # 4) Run your Python script
      #    The script references 'queries/query.sql' and writes to 'output/out.parquet'
      - name: Run Python script
        run: python script/retail_data.py
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}       # Store your keys in GitHub secrets
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: 'ap-south-1'                                  # or your preferred region

      # 5) (Optional) Upload the output parquet file as an artifact
      - name: Upload parquet artifact
        uses: actions/upload-artifact@v3
        with:
          name: athena-parquet-output
          path: output/out.parquet
